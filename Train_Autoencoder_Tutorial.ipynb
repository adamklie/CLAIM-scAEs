{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train an Autoencoder for single-cell RNA-seq data\n",
    "<img src=\"imgs/CLAIM_VanillaAE_basic.png\" class=\"center\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Authorship:**\n",
    "Adam Klie, *01/22/2022*\n",
    "***\n",
    "**Description:**\n",
    "Notebook template for building and training an autoencoder for single-cell omics analysis\n",
    "\n",
    "**Notes:**\n",
    " - **Environment:** You need a Jupyter `python3` kernel with PyTorch and Sklearn installed. [See instructions for setup here.]( https://www.notion.so/Autoencoder-Workshop-73d10091ac014f8c966a503e02759b11)\n",
    " - **GPU Usage:** The default data and model used below are lightweight enough to be trained on a cpu, but if you'd like to train on a larger dataset with more parameters, I would recommend opening a GPU backed notebook. If you are using the `ml_env` kernel described in the above environment setup, you simply need to run the following after logging onto the cluster:\n",
    " \n",
    " ```bash\n",
    " module load cuda10.2\n",
    "jupyter-submit -p carter-gpu -A carter-gpu -t 05-00:00:00 -c 4 -m 16G -g 1 -I\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Packages\n",
    "Here we load the typical base packages we will utilize throughout the exercise. We will also need to import the PyTorch library and check if we are on a GPU node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T06:21:06.386031Z",
     "start_time": "2021-04-26T06:21:06.224110Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  If you are on a GPU, the function will return `True` and will tell you which GPU(s) you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Using a GPU? {}\".format(torch.cuda.is_available()))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device number [0-7]: {}\".format(torch.cuda.current_device()))\n",
    "    print(\"Device count: {}\".format(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can use this variable to keep track of how many epochs you've trained a model for\n",
    "epochs_trained = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "Here we load the preprocessed **pbmc3k** dataset. This dataset captures Peripheral Blood Mononuclear Cells (PBMC) freely available from 10X Genomics. There were originally 2,700 single cells that were sequenced on the Illumina NextSeq 500. Here we load in the raw counts for the variable genes in high quality cells. See the `Collect_Datasets_and_Preprocess.ipynb` and `Collect_Cell_Type_Labels.ipynb` notebooks for more details on the dataset and the preprocessing steps applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw counts of highly variable genes. We have less than 2700 cells due to previous filtering\n",
    "raw_counts = pd.read_csv(\"data/pbmc3k_raw_var_genes.tsv\", index_col=0, sep=\"\\t\")\n",
    "num_genes = raw_counts.shape[0] # rows\n",
    "num_cells = raw_counts.shape[1] # columns\n",
    "print(\"Dataset contains {} genes across {} cells\".format(num_genes, num_cells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standardize inputs\n",
    "Here we scale each genes expression to mean 0 and standard deviation 1 across all cells. This will improve convergence properties during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# StandardScaler prefers samples x features \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(raw_counts.T)\n",
    "scaled_counts = scaler.transform(raw_counts.T)\n",
    "scaled_counts.shape, scaled_counts.mean(axis=0), scaled_counts.std(axis=0)  # Double check scaling was done correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Instantiate the dataloader\n",
    "[DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) are fundamental PyTorch objects that interface the data you want to train with to the model you want to train. A DataLoader is essentially a Python iterator that can be looped through to pull \"batches\" of data at time. These \"batches\" of data are passed to the model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:11:24.609119Z",
     "start_time": "2021-05-20T16:11:24.590436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a TensorDataset object from the scaled array, note that PyTorch likes Tensors, not numpy arrays\n",
    "dataset = TensorDataset(torch.from_numpy(scaled_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:11:57.471895Z",
     "start_time": "2021-05-20T16:11:57.302899Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a simple DataLoaders from the Dataset object\n",
    "loader = DataLoader(dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T16:11:58.556687Z",
     "start_time": "2021-05-20T16:11:58.534431Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check dims of loaders for correctness (should match n_cells x torch.Size(n_genes))\n",
    "print(\"Dimensions of training set: {} x {}\".format(len(loader.dataset), loader.dataset[0][0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The autoencoder model\n",
    "Here we initialize a predefined autoencoder architecture (see `autoencoders.py`). The encoder's job is to take an input vector and output a lower-dimensional latent embedding; the decoder does the opposite, taking the latent embedding to reconstruct/output the input vector. The decoder architecture is a mirror of the encoder's. The input size of the encoder corresponds to the dimensions of our features (variable genes) and outputs a latent embedding of size 10. \n",
    "\n",
    "We initialize the weights using a Kaiming Uniform or He initialization (see `init_weights.py`). Each linear layer is activated with the ReLU function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TODO</b>\n",
    "\n",
    "We provide the most basic autoencoder architecture here. This is your opportunity to explore parameters i.e. the number of hidden layers, the width per layer, etc. or try something completely different!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load predefined model and weight initializer\n",
    "from scripts.utils import init_weights\n",
    "from scripts.autoencoders import VanillaAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate model and move to gpu if available\n",
    "model = VanillaAE(raw_counts.shape[0])\n",
    "model.apply(init_weights)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Executing the model on:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what our model looks like, using a handy summary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will assume a batch size of 256. \n",
    "# The summary function expects you to include the expected input size as a parameter along with a batch size\n",
    "summary(model, input_size=(256, num_genes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's test out our autoencoder structure with our initialized parameters. You can use the following code block to check to see if things are being output the way you would expect. By default, the encoded dimension size should be 10 and the size of the decoded dimesion should match the variable genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab some seqs and outputs to test out on\n",
    "indexes = np.random.choice(scaled_counts.shape[0], size=5)\n",
    "random_cells = torch.from_numpy(scaled_counts[indexes]).float().to(device)\n",
    "\n",
    "# Feed through encoder and to get bottleneck size\n",
    "encoded_outputs = model.encoder(random_cells).squeeze(dim=1)\n",
    "\n",
    "# Feed through encoder and decoder to get full output size\n",
    "outputs = model(random_cells).squeeze(dim=1)\n",
    "print(\"Encoder output dimension: {}\\nDecoder output dimension: {}\".format(encoded_outputs.shape[1], outputs.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set training optimization parameters\n",
    "\n",
    "Before we train our model, we need to instantiate a loss function that we are aiming to optimize and an algorithm for conducting that optimization. Here we use **MSE loss** to model the reconstruction of gene expression counts, and optimize using the **adaptive momentum (Adam) algorithm**. MSE and Adam are popular in deep learning (don't worry, I'm no DanQ), but feel free to play with others or define your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TODO</b>\n",
    "    \n",
    "Try out different optimization strategies and loss functions. This is general to neural networks and not specific to autoencoders, but still an important set of hyperparameters to consider.\n",
    "\n",
    " - PyTorch optimizers: https://pytorch.org/docs/stable/optim.html\n",
    " - PyTorch loss functions: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Here we actually optimize our defined loss function via our autoencoder inputs and reconstructions. We are making use of a training function designed for autoencoders that can be found in the `train.py` file. We will also use the livelossplot package to visualize our loss across training. If all goes well, you should see loss decrease at each epoch (aka iteration through the dataset), something along the lines of:\n",
    "\n",
    "![loss_plot](imgs/example_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.train import train_autoencoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TODO: Make-it-train!</b>\n",
    "    \n",
    "Define the number of epochs and how often to update the loss plot (default every 10 epochs). Note that updating too often will slow down training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "plot_frequency = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make-it-train!\n",
    "tic = time.perf_counter()\n",
    "loss_history, _ = train_autoencoder(model=model, \n",
    "                                    dataloader=loader, \n",
    "                                    criterion=criterion, \n",
    "                                    optimizer=optimizer,\n",
    "                                    device=device,\n",
    "                                    num_epoch=num_epochs,\n",
    "                                    plot_frequency=plot_frequency)\n",
    "toc = time.perf_counter()\n",
    "epochs_trained += num_epochs\n",
    "print(f\"Trained {num_epochs:d} epochs in {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-29T03:15:43.201880Z",
     "iopub.status.busy": "2022-01-29T03:15:43.201643Z",
     "iopub.status.idle": "2022-01-29T03:15:45.325714Z",
     "shell.execute_reply": "2022-01-29T03:15:45.325245Z",
     "shell.execute_reply.started": "2022-01-29T03:15:43.201837Z"
    },
    "tags": []
   },
   "source": [
    "## Save trained model\n",
    "If you are happy with your model, save it's parameters. You can always load it in later for interpretation or to do more training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "torch.save(model.state_dict(), \"models/prelim_model_{}.pt\".format(epochs_trained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualize latent space\n",
    "We are now ready to investigate that latent space our model has learned. We leave coming up with an awesome new analysis to the user, but we wrote some code for you to generate a two dimensional visualization of your latent space using both PCA and UMAP reduction. We added cell-type labels from the [Seurat guided clustering tutorial](https://satijalab.org/seurat/articles/pbmc3k_tutorial.html). Do you see separation between Seurat's cell-type labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TODO: Visualize</b>\n",
    "    \n",
    "Use and modify the `visualize()` function to plot our model's embedding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.utils import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_data = model.encoder(loader.dataset.tensors[0].float().to(device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_ids = [col[0] for col in raw_counts.columns.str.split(\"-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize(latent_embedding=latent_data, cellids=cell_ids, metadata_file=\"data/pbmc3k_SeuratMetadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So how'd you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used Scanpy to download the data, we will start by comparing to them. Here's what Scanpy's tutorial outputs on 40 PCs, 10 nearest neighbors and default UMAP settings. They use the same preprocessing strategy we used for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scanpy_pbm3k](imgs/pbmc3k_100_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Seurat? They have a slightly different pipeline. These were the labels you used for you visualization as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seurat_pbm3k](imgs/pbmc3k_Seurat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adamml",
   "language": "python",
   "name": "adamml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
